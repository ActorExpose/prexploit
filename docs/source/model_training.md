# Model Training

## Build Dataset

Before starting model training, you need to make a dataset from raw data.
This commands extracts features and saves them into a JSON file.
See the [Model Features](./features.md) section for deetails.

```
$ prexploit dataset ./data/nvd ./data/cwe.json ./data/dataset.json
```

## Training and Prediciton

Build a new model using GBDT([Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting) Decision Tree) with [LightGBM](https://github.com/microsoft/LightGBM). Before training,  the SMOTE oversampling is applied for solving data imbalance.

```
$ prexploit train ./data/dataset.json ./data/symantec_cve_list.dat models/model
```

Create data with exploit probability and local explanations based on [SHAP](https://github.com/slundberg/shap).

```
$ prexploit predict ./data/dataset.json ./data/symantec_cve_list.dat ./models/model.zip ../prexploit-data/
```

This command writes new files as bellow. You can see the details in the [Data Feed](./data_feed.md) section.

```
../prexploit-data
├── 2016
├── 2017
├── 2018
├── 2019
└── 2020
    ├── CVE-2020-1.json
    └── CVE-2020-2.json
    └── CVE-2020-3.json
    ├   ...
    ├── CVE-2020-9859.json
    └── CVE-2020.json
```

If you want to maintain your data feed in Github, you can push data as follows. This command squash commit logs to save storage.

```
$ prexploit upload PATH_YOUR_REPO "your commit message"
```

## Evaluate Model

Execute experiments for model evaluation to validate a new model with your dataset and training methods. 
The result of the experiments is saved in a JSON file for comparison of multiple models.

To prevent **data leakage**, the dataset is split based on the date of publishment. Concretely, at first, the first year is used for training and successive half-year is used for testing. After that, the test data is inserted into the training data and the next successive half-year is used for testing. 

```
$ prexploit validate ./data/dataset.json ./data/symantec_cve_list.dat report.json
```

You can show the json results as follows.

```
$ prexploit compare ./report1.json ./report2.json
```

The following example shows a comparison of two models. Training scores very high because of SMOTE oversampling.

```
$ prexploit compare reports/base.json reports/base+FeatureSampling95%.json 
-----------------------  ---------  ------  -------  ---------  ------  -------  ---------  ------  -------
                         Train      Train   Train    Valid      Valid   Valid    Test       Test    Test
                         Precision  Recall  F Score  Precision  Recall  F Score  Precision  Recall  F Score
base                     0.98       0.99    0.99     0.34       0.5     0.4      0.28       0.19    0.16
base+FeatureSampling95%  0.98       0.99    0.99     0.37       0.51    0.43     0.29       0.18    0.15
-----------------------  ---------  ------  -------  ---------  ------  -------  ---------  ------  -------
```