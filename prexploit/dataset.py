import collections
import glob
import json
import os
import zipfile
from functools import lru_cache
import multiprocessing

from tqdm import tqdm
import spacy


try:
    nlp = spacy.load("en_core_web_sm")
except OSError:
    from spacy.cli import download
    download('en_core_web_sm')
    nlp = spacy.load('en_core_web_sm')


nlp.disable_pipes('ner')


def _get_phrases(texts, desc=None):
    delimiter = '_'
    n_cpu = int(multiprocessing.cpu_count() / 2)
    sets = []
    for doc in tqdm(nlp.pipe(texts, n_process=n_cpu, batch_size=50),
                    total=len(texts), desc=desc):
        phrases = [delimiter.join([t.lower_ for t in chunk
                                   if not t.is_stop
                                   if t.is_alpha or t.is_digit])
                   for chunk in doc.noun_chunks]
        sets.append(set(phrases))
    return sets


def load_cwe2data(filepath):
    """Load CWE data from json file

    Args:
        filepath (str): file path to a CWE file

    Returns:
        dict: map from CWE ID to data
    """
    with open(filepath, 'r', encoding='utf-8') as fd:
        cwe2data = json.load(fd)

    cwes = [cwe for cwe in cwe2data.keys()]
    docs = [cwe2data[cwe]['Description'] for cwe in cwes]
    for cwe, phrases in zip(cwes, _get_phrases(docs, 'Parsing CWE Desc')):
        cwe2data[cwe]['Phrases'] = phrases

    return cwe2data


def load_exploit_cves(path_exp_sym):
    """Load Exploted CVEs from an Symantec file

    Args:
        path_exp_sym (str): file path to a Symantec CVEs file

    Returns:
        set: set of exploited CVEs
    """
    with open(os.path.join(path_exp_sym), 'r', encoding='utf-8') as fd:
        exploited_cves = set([row.strip() for row in fd.read().split('\n')
                              if row.strip() != ''])
    return exploited_cves


def _list_cpes(nodes):
    """List all CPEs from configurations.nodes of NVD object

    Node objects has nested CPEs with logical combinations.
    """
    def eval_eq(eq):
        if 'children' in eq:
            return [cpe
                    for child in eq['children']
                    for cpe in eval_eq(child)]
        elif 'cpe_match' in eq:
            return [m['cpe23Uri']
                    for m in eq['cpe_match'] if bool(m['vulnerable'])]
        else:
            return []
    return [cpe for node in nodes for cpe in eval_eq(node)]


class _CPE(object):
    def __init__(self, cpe):
        self.cpe = cpe
        chunks = cpe.split(':')
        self.cpe_version = chunks[1]
        self.type = chunks[2]
        self.vendor = chunks[3]
        self.product = chunks[4]


def _cve_item2features(item, cwe2data):
    features = dict()
    cve = item['cve']['CVE_data_meta']['ID']

    phrases = item['__phrases__']
    features.update({f'T:NP:{p.upper()}': 1 for p in phrases})

    pdata = item['cve']['problemtype']['problemtype_data']
    cwes = [b['value']
            for a in pdata
            for b in a['description']
            if b['value'] != 'NVD-CWE-noinfo']

    @lru_cache(maxsize=1024)
    def get_cwe_features(cwe, is_leaf):
        features = dict()
        if cwe not in cwe2data:
            return features

        if is_leaf:
            features.update({f'C:CWE:{cwe.replace("CWE-", "")}': 1})
        else:
            features.update({f'C:PCWE:{cwe.replace("CWE-", "")}': 1})

        phrases = cwe2data[cwe]['Phrases']
        features.update({f'T:NP:{p.upper()}': 1 for p in phrases})

        for cwe in cwe2data[cwe]['Parent']:
            features.update(get_cwe_features(cwe, False))
        return features

    for cwe in cwes:
        features.update({f: 1 for f in get_cwe_features(cwe, True)})

    cpes = _list_cpes(item['configurations']['nodes'])
    features.update(
        {f'C:CPE_VENDOR:{_CPE(cpe).vendor.upper()}': 1 for cpe in cpes})
    features.update(
        {f'C:CPE_PRODUCT:{_CPE(cpe).product.upper()}': 1 for cpe in cpes})
    cpe_type_dist = collections.Counter([_CPE(cpe).type for cpe in cpes])
    features.update(
        {f'N:CPE_#TYPE:{t.upper()}': d for t, d in cpe_type_dist.items()})

    nvd_pub_date = item['publishedDate'][:10]
    nvd2fname = {
        'attackVector': 'CVSS_AV',
        'attackComplexity': 'CVSS_AC',
        'privilegesRequired': 'CVSS_PR',
        'userInteraction': 'CVSS_UI',
        'scope': 'CVSS_S',
        'confidentialityImpact': 'CVSS_C',
        'integrityImpact': 'CVSS_I',
        'availabilityImpact': 'CVSS_A'
    }
    cvss_v3 = item['impact'].get('baseMetricV3', None)
    if cvss_v3 is None:
        return cve, nvd_pub_date, None
    features.update({f'C:{f}': cvss_v3['cvssV3'][k]
                     for k, f in nvd2fname.items()})
    features['N:CVSS:v3'] = cvss_v3['cvssV3']['baseScore']

    return cve, nvd_pub_date, features


def extract_dataset(nvd_file, cwe2data):
    """Extract dataset from one NVD file.

    Args:
        nvd_file (str): file path to NVD's zip file
        cwe2data (str): file path to a CWE file

    Returns:
        list: list of features's dictionary
    """
    dataset = []
    filename = os.path.basename(nvd_file)[:-4]
    with zipfile.ZipFile(nvd_file) as zf:
        with zf.open(filename, 'r') as fd:
            bundle = json.load(fd)
            docs = [item['cve']['description']['description_data'][0]['value']
                    for item in bundle['CVE_Items']]
            for i, phrases in enumerate(_get_phrases(docs, 'Parsing NVD')):
                bundle['CVE_Items'][i]['__phrases__'] = phrases

            for item in tqdm(bundle['CVE_Items'], desc=f'Parsing {nvd_file}'):
                cve, nvd_pub_date, features =\
                    _cve_item2features(item, cwe2data)
                if features is not None:
                    dataset.append({
                        'id': cve,
                        'date': nvd_pub_date,
                        'features': features
                    })
    return dataset


def build_dataset(path_nvd, path_cwe):
    """Build training dataset from raw data

    Args:
        path_nvd (str): directory path to NVD files
        path_cwe (str): file path to a CWE file

    Returns:
        list: list of features's dictionary
    """
    nvd_files = glob.glob(os.path.join(path_nvd, '*.json.zip'))
    cwe2data = load_cwe2data(path_cwe)

    dataset = []
    for nvd_file in nvd_files:
        dataset.extend(extract_dataset(nvd_file, cwe2data))

    return dataset
