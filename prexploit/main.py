import argparse
import json
import os
import logging

from .dataset import build_dataset, load_exploit_cves
from .model import (load_dataset,
                    make_onlinedata, validate,
                    make_batchdata, build_model,
                    load_model, save_model,
                    write_predict_results)
from .util.report import Report, table_report
from .scheduler import (run_forever, schedule_update_all,
                        schedule_update_modified_cves)
from .data.nvd import download_modified_nvd, download_nvd_history
from .data.symantec import download_symantec_cves
from .data.cwe import download_cwe
from .util.git import squash_and_push

log = logging.getLogger(__name__)


def make_dataset(args):
    data = build_dataset(args.path_nvd, args.path_cwe)
    with open(args.path_out, 'w', encoding='utf-8') as fd:
        json.dump(data, fd, indent=4)


def validate_model(args):
    exploited_cves = load_exploit_cves(args.path_exps)
    dataset = load_dataset(args.path_dataset)
    online_dataset = make_onlinedata(dataset, exploited_cves,
                                     begin_date='2016-01-01',
                                     end_date='2020-04-01')
    report = validate(online_dataset)
    with open(args.path_out, 'w', encoding='utf-8') as fd:
        json.dump(report.as_dict(), fd, indent=4)


def compare_reports(args):
    names = []
    reports = []

    for filepath in args.reports:
        name = os.path.basename(filepath)
        names.append(os.path.splitext(name)[0])
        with open(filepath, 'r', encoding='utf-8') as fd:
            reports.append(Report.from_dict(json.load(fd)))
    table = table_report(reports, names)
    print(table)


def train_model(args):
    if not os.path.exists(args.path_out):
        os.mkdir(args.path_out)

    exploited_cves = load_exploit_cves(args.path_exps)
    dataset = load_dataset(args.path_dataset)
    batch_dataset = make_batchdata(dataset,
                                   exploited_cves,
                                   begin_date=args.begin_date,
                                   end_date=args.end_date)
    vectorizer, classifier = build_model(batch_dataset)
    save_model(args.path_out, vectorizer, classifier)


def predict(args):
    if not os.path.exists(args.path_out):
        raise Exception("Out folder doesn't exists")

    exploited_cves = load_exploit_cves(args.path_exps)
    dataset = load_dataset(args.path_dataset)
    batch_dataset = make_batchdata(dataset,
                                   exploited_cves,
                                   begin_date=args.begin_date,
                                   end_date=args.end_date)
    vectorizer, classifier = load_model(args.path_model)

    cves, x, y = batch_dataset

    x = vectorizer.transform(x)
    explanations = classifier.batch_explain(x, vectorizer.get_feature_names())
    pred_y = classifier.predict_prob(x)
    write_predict_results(cves, pred_y, explanations, args.path_out)


def upload_github(args):
    squash_and_push(args.path_repo, args.msg)


def schedule_jobs(args):
    schedule_update_modified_cves(args.path_nvd, args.path_cwe,
                                  args.path_exps, args.path_model,
                                  args.path_out)

    schedule_update_all(args.path_nvd, args.path_cwe,
                        args.path_exps, args.path_model,
                        args.path_out)
    run_forever()


def download(args):
    if args.path_nvd:
        download_nvd_history(args.path_nvd)
        download_modified_nvd(args.path_nvd)
    if args.path_cwe:
        download_cwe(args.path_cwe)
    if args.path_exps:
        download_symantec_cves(args.path_exps)


def setup_logging(args):
    levels = {
        'DEBUG': logging.DEBUG,
        'INFO': logging.INFO,
        'WARNING': logging.WARNING,
        'ERROR': logging.ERROR,
        'CRITICAL': logging.CRITICAL,
    }

    log_level = logging.ERROR
    if args.log_level is not None:
        log_level = levels[args.log_level.upper()]

    logging.basicConfig(level=log_level)


def main():
    parser = argparse.ArgumentParser(
        description='Prexploit: Open Exploit Prediction System')
    subparsers = parser.add_subparsers()

    parser.add_argument(
        '--log-level', choices=['debug', 'info', 'warning',
                                'error', 'critical'],
        help='Log level',
    )

    parser_make_dataset = subparsers.add_parser(
        'dataset', help='Build dataset from raw data')
    parser_make_dataset.add_argument(
        'path_nvd', action='store', help='Directory path to NVD files',
    )
    parser_make_dataset.add_argument(
        'path_cwe', action='store', help='File path to CWE data',
    )
    parser_make_dataset.add_argument(
        'path_out', action='store', help='File path for a created datset',
    )

    parser_make_dataset.set_defaults(handler=make_dataset)

    parser_train_model = subparsers.add_parser(
        'train', help='Train new model')
    parser_train_model.add_argument(
        'path_dataset', action='store',
        help='File path to training dataset',
    )
    parser_train_model.add_argument(
        'path_exps', action='store',
        help='File path to exploited CVEs',
    )
    parser_train_model.add_argument(
        'path_out', action='store', help='Directory path to store a model',
    )
    parser_train_model.add_argument(
        '--begin-date', default=None,
        action='store', help='Begin date as `2016-01-01` to train',
    )
    parser_train_model.add_argument(
        '--end-date', default=None,
        action='store', help='End date as `2021-01-01` to train',
    )
    parser_train_model.set_defaults(handler=train_model)

    parser_upload = subparsers.add_parser(
        'upload', help='Push all changes to the Github')
    parser_upload.add_argument('path_repo',
                               help='Directory path of a repository')
    parser_upload.add_argument('msg', help='Commit message')
    parser_upload.set_defaults(handler=upload_github)

    parser_predict = subparsers.add_parser(
        'predict', help='Predict exploit probabilities')
    parser_predict.add_argument(
        'path_dataset', action='store',
        help='Directory path to a dataset',
    )
    parser_predict.add_argument(
        'path_exps', action='store',
        help='File path to exploited CVEs',
    )
    parser_predict.add_argument(
        'path_model', action='store', help='File path to a trained model',
    )
    parser_predict.add_argument(
        'path_out', action='store',
        help='Direcoty path to store prediction results',
    )
    parser_predict.add_argument(
        '--begin-date', default=None,
        action='store', help='Begin date as `2016-01-01` to predict',
    )
    parser_predict.add_argument(
        '--end-date', default=None,
        action='store', help='End date as `2021-01-01` to predict',
    )
    parser_predict.set_defaults(handler=predict)

    parser_validate = subparsers.add_parser(
        'validate', help='Validate a model on online data')
    parser_validate.add_argument(
        'path_dataset', action='store',
        help='Directory path to training dataset',
    )
    parser_validate.add_argument(
        'path_exps', action='store',
        help='File path to exploited CVEs',
    )
    parser_validate.add_argument(
        'path_out', action='store', help='File path for validation report',
    )
    parser_validate.set_defaults(handler=validate_model)

    parser_cmp_reports = subparsers.add_parser(
        'compare', help='Compare validation reports')
    parser_cmp_reports.add_argument('reports', nargs='*')
    parser_cmp_reports.set_defaults(handler=compare_reports)

    parser_schedule = subparsers.add_parser(
        'schedule', help='Schedule jobs for the data feed')
    parser_schedule.add_argument(
        'path_nvd', action='store', help='Directory path to NVD files',
    )
    parser_schedule.add_argument(
        'path_cwe', action='store', help='File path to CWE data',
    )
    parser_schedule.add_argument(
        'path_exps', action='store',
        help='Directory path to exploited CVEs',
    )
    parser_schedule.add_argument(
        'path_model', action='store', help='Directory path to store model',
    )
    parser_schedule.add_argument(
        'path_out', action='store',
        help='Direcoty path to store prediction results',
    )
    parser_schedule.set_defaults(handler=schedule_jobs)

    parser_download = subparsers.add_parser(
        'download', help='Download raw data')
    parser_download.add_argument(
        '--path_nvd', action='store', default=None,
        help='Directory path to store NVD files',
    )
    parser_download.add_argument(
        '--path_exps', action='store', default=None,
        help='File path to store exploited CVEs',
    )
    parser_download.add_argument(
        '--path_cwe', action='store', default=None,
        help='File path to store CWE data',
    )
    parser_download.set_defaults(handler=download)

    args = parser.parse_args()
    setup_logging(args)
    if hasattr(args, 'handler'):
        args.handler(args)
    else:
        parser.print_help()


if __name__ == '__main__':
    main()
